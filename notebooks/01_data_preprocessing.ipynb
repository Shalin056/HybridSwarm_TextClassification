{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e2caa75-63fb-4432-9c06-e37b8c94c6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\shali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa9a3792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "\n",
    "# Download NLTK resources (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d860afab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (120000, 3)\n",
      "Test Data Shape: (7600, 3)\n",
      "\n",
      "Training Data Sample:\n",
      "   Class Index                                              Title  \\\n",
      "0            3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
      "1            3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
      "2            3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
      "3            3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
      "4            3  Oil prices soar to all-time record, posing new...   \n",
      "\n",
      "                                         Description  \n",
      "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
      "1  Reuters - Private investment firm Carlyle Grou...  \n",
      "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
      "3  Reuters - Authorities have halted oil export\\f...  \n",
      "4  AFP - Tearaway world oil prices, toppling reco...  \n",
      "\n",
      "Test Data Columns: ['Class Index', 'Title', 'Description']\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "train_path = '../data/raw/AG_news/train.csv'\n",
    "test_path = '../data/raw/AG_news/test.csv'\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Display basic info\n",
    "print(\"Training Data Shape:\", train_df.shape)\n",
    "print(\"Test Data Shape:\", test_df.shape)\n",
    "print(\"\\nTraining Data Sample:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nTest Data Columns:\", train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2147bf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "Preprocessed: reuters short-sellers wall street 's dwindling\\band ultra-cynics seeing green\n"
     ]
    }
   ],
   "source": [
    "# Define preprocessing function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and punctuation\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in punctuation]\n",
    "    # Join tokens back to text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test the function\n",
    "sample_text = train_df['Description'][0]  # Adjust column name if different\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Preprocessed:\", preprocess_text(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd8c0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed Training Data Sample:\n",
      "   Class Index                                     processed_text\n",
      "0            3  reuters short-sellers wall street 's dwindling...\n",
      "1            3  reuters private investment firm carlyle group ...\n",
      "2            3  reuters soaring crude prices plus worries\\abou...\n",
      "3            3  reuters authorities halted oil export\\flows ma...\n",
      "4            3  afp tearaway world oil prices toppling records...\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to training and test data\n",
    "train_df['processed_text'] = train_df['Description'].apply(preprocess_text)  # Adjust column name\n",
    "test_df['processed_text'] = test_df['Description'].apply(preprocess_text)    # Adjust column name\n",
    "\n",
    "# Combine label and processed text for saving\n",
    "train_processed = train_df[['Class Index', 'processed_text']]\n",
    "test_processed = test_df[['Class Index', 'processed_text']]\n",
    "\n",
    "# Display sample of processed data\n",
    "print(\"\\nProcessed Training Data Sample:\")\n",
    "print(train_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39db5f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Training Shape: (120000, 5000)\n",
      "TF-IDF Test Shape: (7600, 5000)\n",
      "Sample Features: ['00' '000' '01' '04' '05' '10' '100' '101' '10th' '11']\n"
     ]
    }
   ],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)  # Limit to 5000 features for efficiency\n",
    "\n",
    "# Fit and transform training data\n",
    "train_tfidf = tfidf.fit_transform(train_processed['processed_text']).toarray()\n",
    "test_tfidf = tfidf.transform(test_processed['processed_text']).toarray()\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "print(\"TF-IDF Training Shape:\", train_tfidf.shape)\n",
    "print(\"TF-IDF Test Shape:\", test_tfidf.shape)\n",
    "print(\"Sample Features:\", feature_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8470e981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to ../data/processed/\n"
     ]
    }
   ],
   "source": [
    "# Define output paths\n",
    "processed_dir = '../data/processed/'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Save preprocessed text\n",
    "train_processed.to_csv(os.path.join(processed_dir, 'train_preprocessed.csv'), index=False)\n",
    "test_processed.to_csv(os.path.join(processed_dir, 'test_preprocessed.csv'), index=False)\n",
    "\n",
    "# Save TF-IDF vectors as pickle files\n",
    "with open(os.path.join(processed_dir, 'train_tfidf.pkl'), 'wb') as f:\n",
    "    pickle.dump(train_tfidf, f)\n",
    "with open(os.path.join(processed_dir, 'test_tfidf.pkl'), 'wb') as f:\n",
    "    pickle.dump(test_tfidf, f)\n",
    "\n",
    "# Save TF-IDF vectorizer for later use\n",
    "with open(os.path.join(processed_dir, 'tfidf_vectorizer.pkl'), 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "print(\"Preprocessed data saved to\", processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88650a74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
