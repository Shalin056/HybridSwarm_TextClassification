{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1013e6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC Path Exists: True\n",
      "SRC Path: C:\\Users\\shali\\Documents\\shalin\\ASU_2nd_SEM\\APM 523 Optimization\\APM523_HybridSwarm_TextClassification\\src\n",
      "Successfully imported models!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set the absolute path to the 'src' directory\n",
    "module_path = r\"C:\\Users\\shali\\Documents\\shalin\\ASU_2nd_SEM\\APM 523 Optimization\\APM523_HybridSwarm_TextClassification\\src\"\n",
    "\n",
    "# Add it to sys.path if it's not already there\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Verify that the path exists\n",
    "print(\"SRC Path Exists:\", os.path.exists(module_path))\n",
    "print(\"SRC Path:\", module_path)\n",
    "\n",
    "# Try importing your models\n",
    "try:\n",
    "    from models import build_lstm_model, build_cnn_model, build_bert_model\n",
    "    print(\"Successfully imported models!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(\"Error importing models:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db9feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from models import build_lstm_model, build_cnn_model, build_bert_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dea9e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF X_train shape: (120000, 5000)\n",
      "TF-IDF X_test shape: (7600, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Load Preprocessed Data\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Add this\n",
    "\n",
    "processed_dir = '../data/processed/'\n",
    "train_tfidf_path = os.path.join(processed_dir, 'train_tfidf.pkl')\n",
    "test_tfidf_path = os.path.join(processed_dir, 'test_tfidf.pkl')\n",
    "train_csv_path = os.path.join(processed_dir, 'train_preprocessed.csv')\n",
    "test_csv_path = os.path.join(processed_dir, 'test_preprocessed.csv')\n",
    "\n",
    "# Load or create TF-IDF data\n",
    "if os.path.exists(train_tfidf_path) and os.path.exists(test_tfidf_path):\n",
    "    with open(train_tfidf_path, 'rb') as f:\n",
    "        X_train_tfidf = pickle.load(f)\n",
    "    with open(test_tfidf_path, 'rb') as f:\n",
    "        X_test_tfidf = pickle.load(f)\n",
    "else:\n",
    "    train_df = pd.read_csv(train_csv_path)\n",
    "    test_df = pd.read_csv(test_csv_path)\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_train_tfidf = vectorizer.fit_transform(train_df['processed_text']).toarray()\n",
    "    X_test_tfidf = vectorizer.transform(test_df['processed_text']).toarray()\n",
    "    with open(train_tfidf_path, 'wb') as f:\n",
    "        pickle.dump(X_train_tfidf, f)\n",
    "    with open(test_tfidf_path, 'wb') as f:\n",
    "        pickle.dump(X_test_tfidf, f)\n",
    "    # Save the vectorizer\n",
    "    with open(os.path.join(processed_dir, 'tfidf_vectorizer.pkl'), 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "y_train = train_df['Class Index'].values - 1\n",
    "y_test = test_df['Class Index'].values - 1\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=4)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes=4)\n",
    "\n",
    "print(\"TF-IDF X_train shape:\", X_train_tfidf.shape)\n",
    "print(\"TF-IDF X_test shape:\", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e63bd9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load raw text for BERT\n",
    "# # Load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# max_length = 128\n",
    "\n",
    "# def tokenize_text(texts, tokenizer, max_length):\n",
    "#     return tokenizer(texts.tolist(), max_length=max_length, padding='max_length', \n",
    "#                      truncation=True, return_tensors='tf')\n",
    "\n",
    "# # Use a subset for faster testing (e.g., 10% of data)\n",
    "# subset_size = int(0.1 * len(train_df))  # 12,000 samples\n",
    "# train_subset_df = train_df.sample(n=subset_size, random_state=42)\n",
    "# test_subset_df = test_df.sample(n=int(0.1 * len(test_df)), random_state=42)  # 760 samples\n",
    "\n",
    "# # Tokenize subset\n",
    "# train_encodings = tokenize_text(train_subset_df['processed_text'], tokenizer, max_length)\n",
    "# test_encodings = tokenize_text(test_subset_df['processed_text'], tokenizer, max_length)\n",
    "\n",
    "# X_train_bert = {'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']}\n",
    "# X_test_bert = {'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']}\n",
    "\n",
    "# # Subset labels\n",
    "# y_train_subset = train_subset_df['Class Index'].values - 1\n",
    "# y_test_subset = test_subset_df['Class Index'].values - 1\n",
    "# y_train_subset_cat = tf.keras.utils.to_categorical(y_train_subset, num_classes=4)\n",
    "# y_test_subset_cat = tf.keras.utils.to_categorical(y_test_subset, num_classes=4)\n",
    "\n",
    "# print(\"BERT X_train input_ids shape:\", X_train_bert['input_ids'].shape)\n",
    "# print(\"BERT X_test input_ids shape:\", X_test_bert['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fa84ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 15ms/step - accuracy: 0.7860 - loss: 0.6428 - val_accuracy: 0.8859 - val_loss: 0.3213\n",
      "Epoch 2/10\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 15ms/step - accuracy: 0.8964 - loss: 0.3317 - val_accuracy: 0.8839 - val_loss: 0.3252\n",
      "Epoch 3/10\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 14ms/step - accuracy: 0.9032 - loss: 0.3023 - val_accuracy: 0.8815 - val_loss: 0.3362\n",
      "Epoch 4/10\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 15ms/step - accuracy: 0.9067 - loss: 0.2804 - val_accuracy: 0.8822 - val_loss: 0.3420\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
      "LSTM - Test Accuracy: 0.8898684210526315\n",
      "LSTM - Test F1-Score: 0.8893417663179489\n"
     ]
    }
   ],
   "source": [
    "# LSTM Training with EarlyStopping\n",
    "input_dim = X_train_tfidf.shape[1]\n",
    "output_dim = 4\n",
    "lstm_default = build_lstm_model(input_dim, output_dim)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history_lstm = lstm_default.fit(X_train_tfidf, y_train_cat, epochs=10, batch_size=32,\n",
    "                               validation_split=0.2, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "y_pred_lstm = lstm_default.predict(X_test_tfidf)\n",
    "y_pred_labels_lstm = np.argmax(y_pred_lstm, axis=1)\n",
    "lstm_accuracy = accuracy_score(y_test, y_pred_labels_lstm)\n",
    "lstm_f1 = f1_score(y_test, y_pred_labels_lstm, average='weighted')\n",
    "print(\"LSTM - Test Accuracy:\", lstm_accuracy)\n",
    "print(\"LSTM - Test F1-Score:\", lstm_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b700fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 76ms/step - accuracy: 0.2502 - loss: 1.3874 - val_accuracy: 0.2446 - val_loss: 1.3856\n",
      "Epoch 2/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 70ms/step - accuracy: 0.2457 - loss: 1.3868 - val_accuracy: 0.2463 - val_loss: 1.3846\n",
      "Epoch 3/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 72ms/step - accuracy: 0.2560 - loss: 1.3855 - val_accuracy: 0.2871 - val_loss: 1.3829\n",
      "Epoch 4/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 72ms/step - accuracy: 0.2575 - loss: 1.3854 - val_accuracy: 0.2912 - val_loss: 1.3814\n",
      "Epoch 5/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 76ms/step - accuracy: 0.2554 - loss: 1.3846 - val_accuracy: 0.2800 - val_loss: 1.3807\n",
      "Epoch 6/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 75ms/step - accuracy: 0.2743 - loss: 1.3844 - val_accuracy: 0.2925 - val_loss: 1.3801\n",
      "Epoch 7/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 73ms/step - accuracy: 0.2788 - loss: 1.3829 - val_accuracy: 0.2950 - val_loss: 1.3799\n",
      "Epoch 8/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 70ms/step - accuracy: 0.2684 - loss: 1.3839 - val_accuracy: 0.3000 - val_loss: 1.3782\n",
      "Epoch 9/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 71ms/step - accuracy: 0.2790 - loss: 1.3817 - val_accuracy: 0.2975 - val_loss: 1.3778\n",
      "Epoch 10/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 70ms/step - accuracy: 0.2778 - loss: 1.3812 - val_accuracy: 0.2971 - val_loss: 1.3767\n",
      "CNN - Test Accuracy: 0.29986842105263156\n",
      "CNN - Test F1-Score: 0.21664738252420798\n"
     ]
    }
   ],
   "source": [
    "# CNN Training with Adjusted Architecture and EarlyStopping\n",
    "subset_size = 12000  # Use 10% subset for faster initial training\n",
    "train_subset_idx = np.random.choice(len(train_df), subset_size, replace=False)\n",
    "X_train_tfidf_subset = X_train_tfidf[train_subset_idx]\n",
    "y_train_cat_subset = y_train_cat[train_subset_idx]\n",
    "\n",
    "cnn_default = build_cnn_model(input_dim, output_dim, filters=64, kernel_size=3)  # Reduced filters and kernel size\n",
    "history_cnn = cnn_default.fit(X_train_tfidf_subset, y_train_cat_subset, epochs=10, batch_size=32,\n",
    "                              validation_split=0.2, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "y_pred_cnn = cnn_default.predict(X_test_tfidf, verbose=0)\n",
    "y_pred_labels_cnn = np.argmax(y_pred_cnn, axis=1)\n",
    "cnn_accuracy = accuracy_score(y_test, y_pred_labels_cnn)\n",
    "cnn_f1 = f1_score(y_test, y_pred_labels_cnn, average='weighted')\n",
    "print(\"CNN - Test Accuracy:\", cnn_accuracy)\n",
    "print(\"CNN - Test F1-Score:\", cnn_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f47b8cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shali\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shali\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 2s/step - accuracy: 0.2580 - loss: 1.5831 - val_accuracy: 0.3900 - val_loss: 1.2958\n",
      "Epoch 2/3\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 2s/step - accuracy: 0.3091 - loss: 1.4523 - val_accuracy: 0.5325 - val_loss: 1.2074\n",
      "Epoch 3/3\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 2s/step - accuracy: 0.3684 - loss: 1.3283 - val_accuracy: 0.6050 - val_loss: 1.1301\n",
      "BERT - Test Accuracy: 0.6275\n",
      "BERT - Test F1-Score: 0.6261429058778213\n"
     ]
    }
   ],
   "source": [
    "# BERT Training with Fine-Tuning and Larger Subset\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_length = 64  # Reduced from 128 for speed\n",
    "\n",
    "def tokenize_text(texts, tokenizer, max_length):\n",
    "    return tokenizer(texts.tolist(), max_length=max_length, padding='max_length', \n",
    "                     truncation=True, return_tensors='tf')\n",
    "\n",
    "subset_size = 2000  # Reduced subset for faster training\n",
    "train_subset_df = train_df.sample(n=subset_size, random_state=42)\n",
    "test_subset_df = test_df  # Full test set for evaluation\n",
    "\n",
    "train_encodings = tokenize_text(train_subset_df['processed_text'], tokenizer, max_length)\n",
    "test_encodings = tokenize_text(test_subset_df['processed_text'], tokenizer, max_length)\n",
    "\n",
    "X_train_bert = {'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']}\n",
    "X_test_bert = {'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']}\n",
    "\n",
    "y_train_subset = train_subset_df['Class Index'].values - 1\n",
    "y_test_subset = test_df['Class Index'].values - 1\n",
    "y_train_subset_cat = tf.keras.utils.to_categorical(y_train_subset, num_classes=4)\n",
    "y_test_subset_cat = tf.keras.utils.to_categorical(y_test_subset, num_classes=4)\n",
    "\n",
    "bert_default = build_bert_model(trainable=False)  # Freeze BERT for speed\n",
    "history_bert = bert_default.fit(X_train_bert, y_train_subset_cat, epochs=3, batch_size=16,  # Smaller batch size\n",
    "                               validation_split=0.2, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "y_pred_bert = bert_default.predict(X_test_bert, verbose=0)\n",
    "y_pred_labels_bert = np.argmax(y_pred_bert, axis=1)\n",
    "bert_accuracy = accuracy_score(y_test_subset, y_pred_labels_bert)\n",
    "bert_f1 = f1_score(y_test_subset, y_pred_labels_bert, average='weighted')\n",
    "print(\"BERT - Test Accuracy:\", bert_accuracy)\n",
    "print(\"BERT - Test F1-Score:\", bert_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b89cd1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and results saved to ../outputs/\n"
     ]
    }
   ],
   "source": [
    "# Save Models and Results (Fixed BERT Saving)\n",
    "output_dir = '../outputs/'\n",
    "models_dir = os.path.join(output_dir, 'models')\n",
    "results_dir = os.path.join(output_dir, 'results')\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "lstm_default.save(os.path.join(models_dir, 'baseline_lstm_default.keras'))\n",
    "cnn_default.save(os.path.join(models_dir, 'baseline_cnn_default.keras'))\n",
    "bert_default.save(os.path.join(models_dir, 'baseline_bert_default.keras'))  # Unified .keras format\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['LSTM', 'CNN', 'BERT (Subset)'],\n",
    "    'Accuracy': [lstm_accuracy, cnn_accuracy, bert_accuracy],\n",
    "    'F1-Score': [lstm_f1, cnn_f1, bert_f1]\n",
    "})\n",
    "results.to_csv(os.path.join(results_dir, 'baseline_results.csv'), index=False)\n",
    "\n",
    "print(\"Models and results saved to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc69ead9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
